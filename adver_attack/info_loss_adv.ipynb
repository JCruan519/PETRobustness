{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from attack_methods import fgsm_attack, pgd_attack\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def patch_wise_drop(image, num_patches, patch_size=16):\n",
    "    # Assuming image is a Tensor of shape (C, H, W)\n",
    "    _, rows, cols = image.shape\n",
    "    patched_image = image.clone()\n",
    "    \n",
    "    # Calculate the number of possible patches in both dimensions\n",
    "    num_possible_patches_x = cols // patch_size\n",
    "    num_possible_patches_y = rows // patch_size\n",
    "    \n",
    "    # Create a grid of all possible patches\n",
    "    all_patches = [(x * patch_size, y * patch_size) for x in range(num_possible_patches_x) for y in range(num_possible_patches_y)]\n",
    "    \n",
    "    # Randomly choose num_patches patches from the list of all possible patches\n",
    "    chosen_patches = torch.randperm(len(all_patches))[:num_patches]\n",
    "    \n",
    "    for index in chosen_patches:\n",
    "        x, y = all_patches[index]\n",
    "        # Since we're dealing with a Tensor, we need to fill the patch with zeros\n",
    "        patched_image[:, y:y + patch_size, x:x + patch_size] = 0  # Assuming that 0 is the value you want to use for masking\n",
    "\n",
    "    return patched_image\n",
    "\n",
    "def pixel_wise_drop(image, num_patches, patch_size=16):\n",
    "    c, h, w = image.shape\n",
    "    total_pixels = h * w\n",
    "    patch_area = patch_size * patch_size\n",
    "    total_patch_area = patch_area * num_patches\n",
    "    drop_ratio = total_patch_area / total_pixels\n",
    "\n",
    "    # Create a flattened array of all pixel indices\n",
    "    pixel_indices = np.arange(h * w)\n",
    "    # Randomly choose a subset of pixel indices to drop\n",
    "    drop_indices = np.random.choice(pixel_indices, int(h * w * drop_ratio), replace=False)\n",
    "\n",
    "    # Create a mask of ones\n",
    "    mask = np.ones((h * w), dtype=np.float32)\n",
    "    # Set chosen pixel indices to zero in the mask to 'drop' them\n",
    "    mask[drop_indices] = 0\n",
    "    mask = mask.reshape((h, w))\n",
    "\n",
    "    # Apply the mask to the image\n",
    "    image_np = image.cpu().numpy().copy()\n",
    "    for i in range(c):\n",
    "        image_np[i] *= mask\n",
    "\n",
    "    return torch.from_numpy(image_np).float()\n",
    "\n",
    "\n",
    "def shuffle_patches(image, grid_size):\n",
    "    \"\"\"\n",
    "    Shuffles patches within an image tensor.\n",
    "\n",
    "    Parameters:\n",
    "    image (Tensor): A PyTorch tensor representing the image.\n",
    "    grid_size (int): The number of patches along one dimension. For example, grid_size=2 creates a 2x2 grid.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: A PyTorch tensor of the shuffled image.\n",
    "    \"\"\"\n",
    "    C, H, W = image.shape\n",
    "    patch_H, patch_W = H // grid_size, W // grid_size\n",
    "\n",
    "    # Check if image dimensions are divisible by grid_size\n",
    "    if H % grid_size != 0 or W % grid_size != 0:\n",
    "        raise ValueError(\"Image dimensions must be divisible by grid_size.\")\n",
    "\n",
    "    # Reshape the image to have dimensions (C, grid_size, patch_H, grid_size, patch_W)\n",
    "    patches = image.view(C, grid_size, patch_H, grid_size, patch_W)\n",
    "    \n",
    "    # Transpose to have dimensions (grid_size, grid_size, C, patch_H, patch_W)\n",
    "    patches = patches.permute(1, 3, 0, 2, 4)\n",
    "    \n",
    "    # Reshape to (grid_size*grid_size, C, patch_H, patch_W) to shuffle\n",
    "    patches = patches.contiguous().view(grid_size * grid_size, C, patch_H, patch_W)\n",
    "    \n",
    "    # Shuffle patches\n",
    "    shuffle_indices = torch.randperm(patches.shape[0])\n",
    "    patches = patches[shuffle_indices]\n",
    "    \n",
    "    # Reshape back to grid\n",
    "    patches = patches.view(grid_size, grid_size, C, patch_H, patch_W)\n",
    "    \n",
    "    # Transpose to (C, grid_size, patch_H, grid_size, patch_W)\n",
    "    patches = patches.permute(2, 0, 3, 1, 4)\n",
    "    \n",
    "    # Reshape back to the original image shape using reshape instead of view\n",
    "    shuffled_image = patches.reshape(C, H, W)\n",
    "    \n",
    "    return shuffled_image\n",
    "\n",
    "\n",
    "def add_gaussian_noise(image, std=0.1, mean=0):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to an image tensor.\n",
    "\n",
    "    Parameters:\n",
    "    image (Tensor): A PyTorch tensor representing the image with shape (C, H, W).\n",
    "    mean (float): The mean of the Gaussian noise distribution.\n",
    "    std (float): The standard deviation of the Gaussian noise distribution.\n",
    "\n",
    "    Returns:\n",
    "    Tensor: A PyTorch tensor of the image with added Gaussian noise.\n",
    "    \"\"\"\n",
    "    noise = torch.randn(image.size()) * std + mean\n",
    "    noisy_image = image + noise\n",
    "    # Clamp the noisy image to be within the valid range [0, 1]\n",
    "    noisy_image = torch.clamp(noisy_image, 0, 1)\n",
    "    return noisy_image\n",
    "\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = []\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.original_size = {}  # 存储原始图像尺寸\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            for line in f:\n",
    "                path, label = line.strip().split()\n",
    "                self.img_labels.append((path, int(label)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx][0])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        self.original_size[idx] = image.size  # 存储原始图像尺寸\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.img_labels[idx][1], idx  # 返回原始尺寸的索引\n",
    "\n",
    "\n",
    "def generate_adversarial_samples(source_root_dir, data_path_names,\n",
    "                                 txt_files, adv_file_name, transform, \n",
    "                                 attck_method, attack_settings):\n",
    "    for i_data_name, dataset_name in enumerate(data_path_names):\n",
    "        print(dataset_name)\n",
    "        \n",
    "        for txt_file in txt_files:\n",
    "            print(txt_file)\n",
    "            annotations_file = os.path.join(source_root_dir, dataset_name, txt_file)\n",
    "            img_dir = os.path.join(source_root_dir, dataset_name)\n",
    "            save_dir = os.path.join(source_root_dir, dataset_name, 'images', adv_file_name)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            dataset = CustomImageDataset(\n",
    "                annotations_file=annotations_file,\n",
    "                img_dir=img_dir,\n",
    "                transform=transform\n",
    "            )\n",
    "            dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "            for images, labels, idxs in tqdm(dataloader):\n",
    "\n",
    "                if attck_method == 'patch_drop':\n",
    "                    # Call the patch_wise_drop function for each image\n",
    "                    adv_images = torch.stack([patch_wise_drop(image, attack_settings['num_patches'], attack_settings['patch_size']) for image in images])\n",
    "                elif attck_method == 'pixel_drop':\n",
    "                    # Call the pixel_wise_drop function for each image\n",
    "                    adv_images = torch.stack([pixel_wise_drop(image, attack_settings['num_patches'], attack_settings['patch_size']) for image in images])\n",
    "                elif attck_method == 'patch_shuffle':\n",
    "                    adv_images = torch.stack([shuffle_patches(image, attack_settings['grid_size']) for image in images])\n",
    "                elif attck_method == 'gaussian_noise':\n",
    "                    adv_images = torch.stack([add_gaussian_noise(image, attack_settings['std']) for image in images])\n",
    "                    \n",
    "                adv_images = adv_images.cpu()\n",
    "\n",
    "                for img, idx in zip(adv_images, idxs):\n",
    "                    original_size = dataset.original_size[idx.item()]\n",
    "                    img_pil = transforms.ToPILImage()(img).resize(original_size)\n",
    "\n",
    "                    img_path = dataset.img_labels[idx.item()][0]\n",
    "                    save_path = os.path.join(save_dir, os.path.basename(img_path))\n",
    "                    img_pil.save(save_path)\n",
    "\n",
    "    print(\"完成对抗样本的生成和保存。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()  # 获取开始时间\n",
    "# 放置你的代码\n",
    "\n",
    "source_root_dir = '/media/ruanjiacheng/新加卷/ecodes/Prompt/data/vtab-1k'\n",
    "data_path_names=(\"caltech101\", \"cifar\", \n",
    "                 \"clevr_count\", \"clevr_dist\", \n",
    "                 \"diabetic_retinopathy\", \"dmlab\", \"dsprites_loc\", \"dsprites_ori\", \n",
    "                 \"dtd\", \"eurosat\", \"oxford_flowers102\", \"kitti\", \"patch_camelyon\", \n",
    "                 \"oxford_iiit_pet\", \"resisc45\", \"smallnorb_azi\", \"smallnorb_ele\", \"sun397\", \"svhn\"\n",
    "                )\n",
    "data_weights_names=(\"caltech101\", \"cifar100\",\n",
    "                    \"clevr_count\", \"clevr_dist\", \n",
    "                 \"diabetic_retinopathy\", \"dmlab\", \"dsprites_loc\", \"dsprites_ori\", \n",
    "                 \"dtd\", \"eurosat\", \"flowers102\", \"kitti\", \"patch_camelyon\", \n",
    "                 \"pets\", \"resisc45\", \"smallnorb_azi\", \"smallnorb_ele\", \"sun397\", \"svhn\"\n",
    "                )\n",
    "dataset_classes=(102, 100,\n",
    "                 8, 6, 5, 6, 16, 16, 47, 10, 102, 4, 2, 37, 45, 18, 9, 397, 10\n",
    "                )\n",
    "# txt_files = ['test.txt', 'train800.txt', 'train800val200.txt', 'val200.txt']\n",
    "txt_files = ['test_adv_500.txt']\n",
    "\n",
    "model_name = 'vit_base_patch16_224_in21k'\n",
    "tuning_mode = ['linear_prob']\n",
    "tuning_coeff=0\n",
    "weight_root_dir='/media/ruanjiacheng/新加卷/ecodes/Prompt/CV/GIST_ALL/outputs_adv/[linear_probe]_0'\n",
    "\n",
    "# 图像转换，无变化\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "attck_method='gaussian_noise'\n",
    "# attack_settings = {\n",
    "#     'num_patches': 140,\n",
    "#     'patch_size': 16\n",
    "# }\n",
    "# attack_settings = {\n",
    "#     'grid_size': 14\n",
    "# }\n",
    "attack_settings = {\n",
    "    'std': 1.0\n",
    "}\n",
    "\n",
    "# adv_file_name = f'{attck_method}_num-{attack_settings[\"num_patches\"]}_size-{attack_settings[\"patch_size\"]}'\n",
    "adv_file_name = f'{attck_method}std-{attack_settings[\"std\"]}'\n",
    "\n",
    "\n",
    "generate_adversarial_samples(source_root_dir, data_path_names,\n",
    "                                 txt_files, adv_file_name, transform, \n",
    "                                 attck_method, attack_settings)\n",
    "\n",
    "end_time = time.time()  # 获取结束时间\n",
    "print(f\"执行时间：{end_time - start_time} 秒\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
