{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define two different nn.Linear layers\n",
    "linear1 = nn.Linear(10, 5)\n",
    "linear2 = nn.Linear(10, 5)\n",
    "\n",
    "# Randomly initialize weights\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "\n",
    "# Function to compute pairwise cosine similarity\n",
    "def pairwise_cosine_similarity(layer1, layer2):\n",
    "    normalized_layer1 = layer1.weight / layer1.weight.norm(dim=1)[:, None]\n",
    "    normalized_layer2 = layer2.weight / layer2.weight.norm(dim=1)[:, None]\n",
    "    similarity = torch.mm(normalized_layer1, normalized_layer2.T)\n",
    "    return similarity\n",
    "\n",
    "# Calculate pairwise cosine similarity\n",
    "similarity_matrix = pairwise_cosine_similarity(linear1, linear2)\n",
    "\n",
    "# Visualize the similarity matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_matrix.detach().numpy(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Pairwise Cosine Similarity between Two nn.Linear Layers\")\n",
    "plt.xlabel(\"Linear Layer 2 Neurons\")\n",
    "plt.ylabel(\"Linear Layer 1 Neurons\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Create two linear layers\n",
    "linear1 = nn.Linear(10, 5)\n",
    "linear2 = nn.Linear(10, 5)\n",
    "\n",
    "# Extract their weights\n",
    "weights1 = linear1.weight.data.numpy().flatten()\n",
    "weights2 = linear2.weight.data.numpy().flatten()\n",
    "\n",
    "# Plot the distributions of the weights\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(weights1, kde=True, color='blue', label='Linear Layer 1')\n",
    "plt.title('Weights Distribution of Linear Layer 1')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(weights2, kde=True, color='orange', label='Linear Layer 2')\n",
    "plt.title('Weights Distribution of Linear Layer 2')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Wasserstein distance (Earth Mover's distance) between the two distributions\n",
    "distance = wasserstein_distance(weights1, weights2)\n",
    "print(\"Wasserstein Distance:\", distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Create two linear layers\n",
    "linear1 = nn.Linear(10, 5)\n",
    "linear2 = nn.Linear(10, 5)\n",
    "\n",
    "# Extract their weights\n",
    "weights1 = linear1.weight.data.numpy().flatten()\n",
    "weights2 = linear2.weight.data.numpy().flatten()\n",
    "\n",
    "# Plot the distributions of the weights on the same plot with different colors\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(weights1, kde=True, color='blue', label='Linear Layer 1')\n",
    "sns.histplot(weights2, kde=True, color='orange', label='Linear Layer 2')\n",
    "plt.title('Weights Distribution of Linear Layers')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Wasserstein distance (Earth Mover's distance) between the two distributions\n",
    "distance = wasserstein_distance(weights1, weights2)\n",
    "print(\"Wasserstein Distance:\", distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "# Create two linear layers\n",
    "linear1 = nn.Linear(10, 5)\n",
    "linear2 = nn.Linear(10, 5)\n",
    "\n",
    "# Extract their weights\n",
    "weights1 = linear1.weight.data.numpy().flatten()\n",
    "weights2 = linear2.weight.data.numpy().flatten()\n",
    "\n",
    "# Plot the distributions of the weights on the same graph with filled area\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.kdeplot(weights1, color='blue', label='Linear Layer 1', fill=True, alpha=0.5)\n",
    "sns.kdeplot(weights2, color='orange', label='Linear Layer 2', fill=True, alpha=0.5)\n",
    "\n",
    "plt.title('Weights Distribution Comparison')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Wasserstein distance (Earth Mover's distance) between the two distributions\n",
    "distance = wasserstein_distance(weights1, weights2)\n",
    "print(\"Wasserstein Distance:\", distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "#---------- adaptformer wad used! ----------#\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "## gist_seq_adapter\n",
    "# model_b_path = '/media/ruanjiacheng/新加卷/ecodes/Prompt/CV/GIST_KD/outputs/ablation-adapter-b/vtab/cifar_100/gist_seq_adapter/20231217-184930-vit_base_patch16_224_in21k-224/model_best.pth.tar'\n",
    "# model_s_path = '/media/ruanjiacheng/新加卷/ecodes/Prompt/CV/GIST_KD/outputs/adapter-s/vtab/cifar_100/gist_seq_adapter/20231217-190830-vit_small_patch16_224_in21k-224/model_best.pth.tar'\n",
    "# model_l_path = '/media/ruanjiacheng/新加卷/ecodes/Prompt/CV/GIST_KD/outputs/adapter-l/vtab/cifar_100/gist_seq_adapter/20231217-175405-vit_large_patch16_224_in21k-224/model_best.pth.tar'\n",
    "# model_lb_path = '/media/ruanjiacheng/新加卷/ecodes/Prompt/CV/GIST_KD/outputs/ablation-adapter-b/vtab/cifar_100/gist_seq_adapter/20231217-172257-vit_base_patch16_224_in21k-vit_large_patch16_224_in21k-224/model_best.pth.tar'\n",
    "# model_sb_path = '/media/ruanjiacheng/新加卷/ecodes/Prompt/CV/GIST_KD/outputs/ablation-adapter-b/vtab/cifar_100/gist_seq_adapter/20231217-191707-vit_base_patch16_224_in21k-vit_small_patch16_224_in21k-224/model_best.pth.tar'\n",
    "# model_bb_path = '/media/ruanjiacheng/新加卷/ecodes/Prompt/CV/GIST_KD/outputs/ablation-adapter-b/vtab/cifar_100/gist_seq_adapter/20231217-193924-vit_base_patch16_224_in21k-vit_base_patch16_224_in21k-224/model_best.pth.tar'\n",
    "\n",
    "## adaptformer\n",
    "model_b_path = '/media/ruanjiacheng/新加卷/ecodes/Prompt/CV/GIST_KD/outputs_ablation/ablation_b/vtab/cifar_100/adaptformer/b/model_best.pth.tar'\n",
    "model_s_path = '/media/ruanjiacheng/新加卷/ecodes/Prompt/CV/GIST_KD/outputs_ablation/s/vtab/cifar_100/adaptformer/20231215-140832-vit_small_patch16_224_in21k-224/model_best.pth.tar'\n",
    "model_l_path = '/media/ruanjiacheng/新加卷/ecodes/Prompt/CV/GIST_KD/outputs_ablation/l/vtab/cifar_100/adaptformer/20231215-125414-vit_large_patch16_224_in21k-224/model_best.pth.tar'\n",
    "model_lb_path = '/media/ruanjiacheng/新加卷/ecodes/Prompt/CV/GIST_KD/outputs_ablation/ablation_b/vtab/cifar_100/adaptformer/lb_kl/model_best.pth.tar'\n",
    "model_sb_path = '/media/ruanjiacheng/新加卷/ecodes/Prompt/CV/GIST_KD/outputs_ablation/ablation_b/vtab/cifar_100/adaptformer/sb_kl/model_best.pth.tar'\n",
    "model_bb_path = '/media/ruanjiacheng/新加卷/ecodes/Prompt/CV/GIST_KD/outputs_ablation/ablation_b/vtab/cifar_100/adaptformer/bb_kl/model_best.pth.tar'\n",
    "\n",
    "\n",
    "\n",
    "tuning_mode='adaptformer'\n",
    "num_class = 100\n",
    "dataset='cifar_100'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from timm.models import create_model\n",
    "from models import vision_transformer_gist_adapter_plot\n",
    "from data import create_loader, create_dataset\n",
    "import cv2\n",
    "\n",
    "\n",
    "# 加载预训练的 ViT 模型\n",
    "model_b = create_model(\n",
    "    'vit_base_patch16_224_in21k',\n",
    "    pretrained=False,\n",
    "    num_classes=num_class,\n",
    "    scriptable=True,\n",
    "    checkpoint_path=model_b_path,\n",
    "    tuning_mode=tuning_mode)\n",
    "model_b = model_b.to('cuda')  # 将模型移动到CUDA设备上\n",
    "model_b.eval()\n",
    "\n",
    "model_sb = create_model(\n",
    "    'vit_base_patch16_224_in21k',\n",
    "    pretrained=False,\n",
    "    num_classes=num_class,\n",
    "    scriptable=True,\n",
    "    checkpoint_path=model_sb_path,\n",
    "    tuning_mode=tuning_mode)\n",
    "model_sb = model_sb.to('cuda')  # 将模型移动到CUDA设备上\n",
    "model_sb.eval()\n",
    "\n",
    "model_lb = create_model(\n",
    "    'vit_base_patch16_224_in21k',\n",
    "    pretrained=False,\n",
    "    num_classes=num_class,\n",
    "    scriptable=True,\n",
    "    checkpoint_path=model_lb_path,\n",
    "    tuning_mode=tuning_mode)\n",
    "model_lb = model_lb.to('cuda')  # 将模型移动到CUDA设备上\n",
    "model_lb.eval()\n",
    "\n",
    "model_bb = create_model(\n",
    "    'vit_base_patch16_224_in21k',\n",
    "    pretrained=False,\n",
    "    num_classes=num_class,\n",
    "    scriptable=True,\n",
    "    checkpoint_path=model_bb_path,\n",
    "    tuning_mode=tuning_mode)\n",
    "model_bb = model_bb.to('cuda')  # 将模型移动到CUDA设备上\n",
    "model_bb.eval()\n",
    "\n",
    "\n",
    "model_s = create_model(\n",
    "    'vit_small_patch16_224_in21k',\n",
    "    pretrained=False,\n",
    "    num_classes=num_class,\n",
    "    scriptable=True,\n",
    "    checkpoint_path=model_s_path,\n",
    "    tuning_mode=tuning_mode)\n",
    "model_s = model_s.to('cuda')  # 将模型移动到CUDA设备上\n",
    "model_s.eval()\n",
    "\n",
    "model_l = create_model(\n",
    "    'vit_large_patch16_224_in21k',\n",
    "    pretrained=False,\n",
    "    num_classes=num_class,\n",
    "    scriptable=True,\n",
    "    checkpoint_path=model_l_path,\n",
    "    tuning_mode=tuning_mode)\n",
    "model_l = model_l.to('cuda')  # 将模型移动到CUDA设备上\n",
    "model_l.eval()\n",
    "\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def plot_weight_distributions(linear1, linear2, linear3, label1, label2, label3):\n",
    "    # Set the plot style\n",
    "    sns.set_context(\"paper\")\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # Extract weights\n",
    "    weights1 = linear1.weight.data.cpu().numpy().flatten()\n",
    "    weights2 = linear2.weight.data.cpu().numpy().flatten()\n",
    "    weights3 = linear3.weight.data.cpu().numpy().flatten()\n",
    "\n",
    "    # Plot the distributions of the weights on the same graph with filled area\n",
    "    plt.figure(figsize=(8, 6), dpi=300)\n",
    "\n",
    "    sns.kdeplot(weights1, color='darkgreen', label=label1, fill=True, alpha=0.35)\n",
    "    sns.kdeplot(weights2, color='darkred', label=label2, fill=True, alpha=0.35)\n",
    "    sns.kdeplot(weights3, color='darkblue', label=label3, fill=True, alpha=0.35)\n",
    "\n",
    "    # plt.title('Weights Distribution Comparison')\n",
    "    plt.xlabel('Weight', fontsize=14)\n",
    "    plt.ylabel('Density', fontsize=14)\n",
    "    plt.gca().xaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "    plt.gca().yaxis.set_major_locator(ticker.MultipleLocator(4))\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.legend()\n",
    "    distance1 = wasserstein_distance(weights1, weights2)\n",
    "    distance2 = wasserstein_distance(weights1, weights3)\n",
    "    distance3 = wasserstein_distance(weights2, weights3)\n",
    "    print(distance1,distance2,distance3)\n",
    "    # sns.despine(trim=True)\n",
    "\n",
    "    # Save the plot to a file in vector format\n",
    "    SAVE_FIG_PATH = f'FIGS/' + label1+label2+label3+'S-B_'+str({distance1:.5})+'S-L_'+str({distance2:.5})+'B-L_'+str({distance3:.5})+'.pdf'\n",
    "    plt.savefig(SAVE_FIG_PATH, format='pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.close()  # Close the plot to free up memory\n",
    "\n",
    "# Example usage\n",
    "# plot_weight_distributions(linear1, linear2, linear3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot head fig; only 2 distribution\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def plot_weight_distributions(linear1_list, linear2_list, linear3_list, label1_list, label2_list, label3_list):\n",
    "    sns.set_context(\"paper\")\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    # Create a figure with subplots in 1 row and 2 columns\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 3), dpi=300)\n",
    "\n",
    "    for i in range(2):\n",
    "        # Extract weights for the current set\n",
    "        weights1 = linear1_list[i].weight.data.cpu().numpy().flatten()\n",
    "        weights2 = linear2_list[i].weight.data.cpu().numpy().flatten()\n",
    "        weights3 = linear3_list[i].weight.data.cpu().numpy().flatten()\n",
    "\n",
    "        # Plot the distributions of the weights\n",
    "        sns.kdeplot(weights1, color='darkgreen', label=label1_list[i], fill=True, alpha=0.35, ax=axes[i])\n",
    "        sns.kdeplot(weights2, color='darkred', label=label2_list[i], fill=True, alpha=0.35, ax=axes[i])\n",
    "        sns.kdeplot(weights3, color='darkblue', label=label3_list[i], fill=True, alpha=0.35, ax=axes[i])\n",
    "\n",
    "        # Plot the distributions of the weights\n",
    "        # sns.kdeplot(weights1, color='darkgreen', fill=True, alpha=0.35, ax=axes[i])\n",
    "        # sns.kdeplot(weights2, color='darkred', fill=True, alpha=0.35, ax=axes[i])\n",
    "        # sns.kdeplot(weights3, color='darkblue', fill=True, alpha=0.35, ax=axes[i])\n",
    "\n",
    "        axes[i].set_xlabel('Weight', fontsize=12)\n",
    "        axes[i].set_ylabel('Density', fontsize=12)\n",
    "        axes[i].xaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "        axes[i].yaxis.set_major_locator(ticker.MultipleLocator(4))\n",
    "        axes[i].tick_params(axis='both', which='major', labelsize=10)\n",
    "        axes[i].legend()\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to a file in vector format\n",
    "    plt.savefig('output.pdf', format='pdf', dpi=600, bbox_inches='tight')\n",
    "    plt.close()  # Close the plot to free up memory\n",
    "\n",
    "linear1_list=[]\n",
    "linear2_list=[]\n",
    "linear3_list=[]\n",
    "label1_list=[]\n",
    "label2_list=[]\n",
    "label3_list=[]\n",
    "\n",
    "for i in range(len(model_b.blocks)):\n",
    "    linear1 = model_s.blocks[i].gist_adapter.linear1\n",
    "    linear1_list.append(linear1)\n",
    "    linear2 = model_b.blocks[i].gist_adapter.linear1\n",
    "    linear2_list.append(linear2)\n",
    "    linear3 = model_l.blocks[int(i*2)].gist_adapter.linear1\n",
    "    linear3_list.append(linear3)\n",
    "    # linear3 = model_sb.blocks[i].gist_adapter.linear1\n",
    "    # linear3_list.append(linear3)\n",
    "\n",
    "    linear1 = model_s.blocks[i].gist_adapter.linear2\n",
    "    linear1_list.append(linear1)\n",
    "    linear2 = model_b.blocks[i].gist_adapter.linear2\n",
    "    linear2_list.append(linear2)\n",
    "    linear3 = model_l.blocks[int(i*2)].gist_adapter.linear2\n",
    "    linear3_list.append(linear3)\n",
    "    # linear3 = model_sb.blocks[i].gist_adapter.linear2\n",
    "    # linear3_list.append(linear3)\n",
    "\n",
    "    label1 = 'W_down in Adapter of ViT-S'\n",
    "    label1_list.append(label1)\n",
    "    label2 = 'W_down in Adapter of ViT-B'\n",
    "    label2_list.append(label2)\n",
    "    label3 = 'W_down in Adapter of ViT-L'\n",
    "    label3_list.append(label3)\n",
    "    # label3 = 'W_down in Adapter of [ViT-S --> ViT-B]'\n",
    "    # label3_list.append(label3)\n",
    "\n",
    "    label1 = 'W_up in Adapter of ViT-S'\n",
    "    label1_list.append(label1)\n",
    "    label2 = 'W_up in Adapter of ViT-B'\n",
    "    label2_list.append(label2)\n",
    "    label3 = 'W_up in Adapter of ViT-L'\n",
    "    label3_list.append(label3)\n",
    "    # label3 = 'W_up in Adapter of [ViT-S --> ViT-B]'\n",
    "    # label3_list.append(label3)\n",
    "\n",
    "    if i == 0: break\n",
    "\n",
    "\n",
    "plot_weight_distributions(linear1_list, linear2_list, linear3_list, label1_list, label2_list, label3_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot all s b l s-b\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def plot_weight_distributions(linear1_list, linear2_list, linear3_list, linear4_list, label1_list, label2_list, label3_list, label4_list):\n",
    "    # Set the plot style\n",
    "    sns.set_context(\"paper\")\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # Create a figure with subplots in 2 rows and 6 columns\n",
    "    fig, axes = plt.subplots(2, 6, figsize=(30, 10), dpi=300)\n",
    "    \n",
    "    for i in range(12):\n",
    "        # Determine the row and column index\n",
    "        row = i // 6\n",
    "        col = i % 6\n",
    "\n",
    "        # Extract weights for the current set\n",
    "        weights1 = linear1_list[i].weight.data.cpu().numpy().flatten()\n",
    "        weights2 = linear2_list[i].weight.data.cpu().numpy().flatten()\n",
    "        weights3 = linear3_list[i].weight.data.cpu().numpy().flatten()\n",
    "        # weights4 = linear4_list[i].weight.data.cpu().numpy().flatten()\n",
    "\n",
    "        # Plot the distributions of the weights\n",
    "        sns.kdeplot(weights1, color='darkgreen', fill=True, label=label1_list[i], alpha=0.35, ax=axes[row, col])\n",
    "        sns.kdeplot(weights2, color='darkred', fill=True, label=label2_list[i], alpha=0.35, ax=axes[row, col])\n",
    "        sns.kdeplot(weights3, color='darkblue', fill=True, label=label3_list[i], alpha=0.35, ax=axes[row, col])\n",
    "        # sns.kdeplot(weights4, color='darkblue', fill=True, label=label4_list[i], alpha=0.35, ax=axes[row, col])\n",
    "\n",
    "        axes[row, col].set_xlabel('Weight', fontsize=16)\n",
    "        axes[row, col].set_ylabel('Density', fontsize=16)\n",
    "        axes[row, col].xaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "        axes[row, col].yaxis.set_major_locator(ticker.MultipleLocator(4))\n",
    "        axes[row, col].tick_params(axis='both', which='major', labelsize=10)\n",
    "        axes[row, col].legend(fontsize=10)\n",
    "\n",
    "        # axes[row, col].xaxis.set_visible(False)\n",
    "        # axes[row, col].yaxis.set_visible(False)\n",
    "        # axes[row, col].xaxis.label.set_visible(False)\n",
    "        # axes[row, col].yaxis.label.set_visible(False)\n",
    "        # axes[row, col].grid(color='r', linestyle='-', linewidth=0.5)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to a file in vector format\n",
    "    plt.savefig('output.pdf', format='pdf', dpi=600, bbox_inches='tight')\n",
    "    plt.close()  # Close the plot to free up memory\n",
    "\n",
    "\n",
    "\n",
    "linear1_list=[]\n",
    "linear2_list=[]\n",
    "linear3_list=[]\n",
    "linear4_list=[]\n",
    "label1_list=[]\n",
    "label2_list=[]\n",
    "label3_list=[]\n",
    "label4_list=[]\n",
    "\n",
    "for i in range(len(model_b.blocks)):\n",
    "    linear1 = model_s.blocks[i].gist_adapter.linear1\n",
    "    linear1_list.append(linear1)\n",
    "    linear2 = model_b.blocks[i].gist_adapter.linear1\n",
    "    linear2_list.append(linear2)\n",
    "    linear3 = model_l.blocks[int(i*2)].gist_adapter.linear1\n",
    "    linear3_list.append(linear3)\n",
    "    linear4 = model_sb.blocks[int(i)].gist_adapter.linear1\n",
    "    linear4_list.append(linear4)\n",
    "\n",
    "    label1 = 'ViT-S' + '_' + str(i) + '_' + 'W_down'\n",
    "    label1_list.append(label1)\n",
    "    label2 = 'ViT-B' + '_' + str(i) + '_' + 'W_down'\n",
    "    label2_list.append(label2)\n",
    "    label3 = 'ViT-L' + '_' + str(int(i*2)) + '_' + 'W_down'\n",
    "    label3_list.append(label3)\n",
    "    label4 = 'ViT-S' + '-->' + 'ViT-B_' + str(int(i)) + '_' + 'W_down'\n",
    "    label4_list.append(label4)\n",
    "\n",
    "\n",
    "plot_weight_distributions(linear1_list, linear2_list, linear3_list, linear4_list, label1_list, label2_list, label3_list, label4_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1_list=[]\n",
    "linear2_list=[]\n",
    "linear3_list=[]\n",
    "linear4_list=[]\n",
    "label1_list=[]\n",
    "label2_list=[]\n",
    "label3_list=[]\n",
    "label4_list=[]\n",
    "\n",
    "for i in range(len(model_b.blocks)):\n",
    "    linear1 = model_s.blocks[i].gist_adapter.linear2\n",
    "    linear1_list.append(linear1)\n",
    "    linear2 = model_b.blocks[i].gist_adapter.linear2\n",
    "    linear2_list.append(linear2)\n",
    "    linear3 = model_l.blocks[int(i*2)].gist_adapter.linear2\n",
    "    linear3_list.append(linear3)\n",
    "    linear4 = model_sb.blocks[int(i)].gist_adapter.linear2\n",
    "    linear4_list.append(linear4)\n",
    "\n",
    "    label1 = 'ViT-S' + '_' + str(i) + '_' + 'W_up'\n",
    "    label1_list.append(label1)\n",
    "    label2 = 'ViT-B' + '_' + str(i) + '_' + 'W_up'\n",
    "    label2_list.append(label2)\n",
    "    label3 = 'ViT-L' + '_' + str(int(i*2)) + '_' + 'W_up'\n",
    "    label3_list.append(label3)\n",
    "    label4 = 'ViT-S' + '-->' + 'ViT-B_' + str(int(i)) + '_' + 'W_up'\n",
    "    label4_list.append(label4)\n",
    "\n",
    "\n",
    "plot_weight_distributions(linear1_list, linear2_list, linear3_list, linear4_list, label1_list, label2_list, label3_list, label4_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1387.4000000000005"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "73.9 \t+92.8 \t+76.9 \t+99.6 \t+92.3 \t+84.0 \t+54.9 \t+85.1 \t+96.9 \t+88.6 \t+76.2 \t+83.7 \t+60.4 \t+53.4 \t+77.4 \t+70.7 \t+53.2 \t+27.2 \t+40.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440.1000000000001"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "74.1 +94.8 +71.8 +99.4 +91.7 +90.4 +57.2 +87.9 +96.7 +87.5 +74.8 +81.9 +64.7 +51.5 +81.9 +93.9 +54.0 +35.6 +50.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.79473684210527"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1440.1000000000001/19"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
